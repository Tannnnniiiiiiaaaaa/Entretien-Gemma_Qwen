import os
import fitz  # PyMuPDF
from dotenv import load_dotenv
from openai import OpenAI
import time
import json

# 1. Charger les variables d'environnement (.env)
load_dotenv()

# 2. Lire chemins vers les fichiers PDF
pdf_keys = [f"PDF_{i}" for i in range(1, 30)]  # 29 entretiens
pdf_paths = [os.getenv(k) for k in pdf_keys]

# 3. V√©rifier que les fichiers existent
for idx, path in enumerate(pdf_paths, 1):
    if not path or not os.path.exists(path):
        raise ValueError(f"‚ùå Fichier PDF introuvable pour Entretien {idx} : {path}")

# 4. Initialiser le client Groq
client = OpenAI(
    base_url="https://api.groq.com/openai/v1",
    api_key=os.getenv("GROQ_API_KEY"),
)

# 5. Fonction pour lire un PDF
def lire_pdf(chemin):
    texte = ""
    with fitz.open(chemin) as doc:
        for page in doc:
            texte += page.get_text()
    return texte

# 6. Fonction pour analyser un morceau de texte
def analyser_entretien_texte(entretien_texte, idx):
    prompt = f"""
Tu es un sociologue expert en analyse qualitative.

Voici un extrait d'entretien r√©alis√© avec un √©tudiant :

\"\"\"{entretien_texte}\"\"\"

Ta t√¢che :
- Identifie des th√®mes g√©n√©raux pertinents,
- Pour chaque th√®me, donne un sous-th√®me sp√©cifique,
- Et pour chaque sous-th√®me, donne un verbatim exact (citation tir√©e du texte).

R√©ponds uniquement avec le format suivant (sans introduction, sans analyse), 3 √† 5 blocs maximum :

Th√®me : [nom du th√®me]  
Sous-th√®me : [nom du sous-th√®me]  
Verbatim : "Phrase compl√®te tir√©e du texte, sans la tronquer, m√™me si elle est longue."

Important :
- N'utilise pas de format JSON.
- Ne d√©passe pas 500 tokens.
- Reste concis et clair.
"""
    try:
        response = client.chat.completions.create(
            model="qwen-qwq-32b",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
            max_tokens=800
        )
        return response.choices[0].message.content.strip()

    except Exception as e:
        print(f"‚õî Erreur API pour Entretien {idx} : {e}")
        if "rate limit" in str(e).lower():
            print("‚è≥ Attente 65 sec (limite atteinte)")
            time.sleep(65)
        return f"[Erreur lors de l'analyse : {e}]"

# 7. Cr√©er dossier de r√©sultats
os.makedirs("resultats_qwen", exist_ok=True)

# 8. Boucle sur les entretiens
#for idx, path in enumerate(pdf_paths, 1):
for idx, path in [(27, pdf_paths[26])]:
    print(f"\nüîç Analyse de l'entretien {idx}...")

    texte = lire_pdf(path)

    # Diviser en morceaux de 1000 caract√®res
    morceaux = [texte[i:i+1500] for i in range(0, len(texte), 1500)]
    # Limiter le nombre de morceaux √† 3 maximum
    morceaux = morceaux[:5]
    analyses = []

    for i, morceau in enumerate(morceaux):
        print(f"‚Üí Analyse du morceau {i+1} / {len(morceaux)}")
        reponse = analyser_entretien_texte(morceau, idx)

        # S√©parer chaque bloc (par th√®me) en listant proprement
        blocs = [bloc.strip() for bloc in reponse.split("Th√®me : ") if bloc.strip()]
        for bloc in blocs:
            lignes = bloc.splitlines()
            theme = sous_theme = verbatim = None
            for ligne in lignes:
                if ligne.startswith("Sous-th√®me"):
                    sous_theme = ligne.replace("Sous-th√®me :", "").strip()
                elif ligne.startswith("Verbatim"):
                    verbatim = ligne.replace("Verbatim :", "").strip().strip('"')
                else:
                    theme = ligne.strip()
            if theme and sous_theme and verbatim:
                analyses.append({
                    "theme": theme,
                    "sous_theme": sous_theme,
                    "verbatim": verbatim
                })
        time.sleep(17)

    # Enregistrer dans un fichier JSON
    fichier_resultat = f"resultats_qwen/Entretien_{idx}.json"
    with open(fichier_resultat, "w", encoding="utf-8") as f:
        json.dump(analyses, f, indent=2, ensure_ascii=False)

    print(f"‚úÖ R√©sultat enregistr√© dans : {fichier_resultat}")

print("\nüéâ Tous les entretiens ont √©t√© analys√©s et enregistr√©s dans le dossier 'resultats'.")